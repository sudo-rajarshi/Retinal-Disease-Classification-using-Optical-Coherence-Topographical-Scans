{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Image Classifier Using Convolution Neural Network\n",
    "This software automatically builds classification model for n-number of classes and no need to write a single line of code. The user just need to throw a dataset arranged in a very simple manner. The state of the art algorithms automatically normalizes the dataset according to the data and train it accordingly.\n",
    "\n",
    "**Install these libraries:-**\n",
    "1. Keras\n",
    "2. Tensorflow\n",
    "3. OpenCV\n",
    "4. Matplotlib\n",
    "5. Shutil\n",
    "6. gTTS\n",
    "7. playsound\n",
    "8. cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "You can skip this part as this part just reads out and print the instructions and has no connection with the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELCOME TO SMART CLASSIFIER USING CONVOLUTION NEURAL NETWORK\n",
      "\n",
      "\n",
      "* Here you can classify 'n' number of classes using different models.\n",
      "* You can also create your own model with all possibe fine tuning.\n",
      "\n",
      "How to arrange tha dataset?\n",
      "\n",
      "If you have one folder containg all the images of different classes together, then-\n",
      "1. Differentiate all images of each classes and store them in different folders.\n",
      "2. Then store all the folder of different classes to a new mother folder.\n",
      "3. Store different images other than training images to a seperate folder that will be used for testing.\n",
      "\n",
      "Please read the above lines and then press ENTER to continue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"WELCOME TO SMART CLASSIFIER USING CONVOLUTION NEURAL NETWORK\\n\")\n",
    "print(\"\\n* Here you can classify 'n' number of classes using different models.\")\n",
    "print(\"* You can also create your own model with all possibe fine tuning.\")\n",
    "print(\"\\nHow to arrange tha dataset?\")\n",
    "print(\"\\nIf you have one folder containg all the images of different classes together, then-\")\n",
    "print(\"1. Differentiate all images of each classes and store them in different folders.\")\n",
    "print(\"2. Then store all the folder of different classes to a new mother folder.\")\n",
    "print(\"3. Store different images other than training images to a seperate folder that will be used for testing.\")\n",
    "input(\"\\nPlease read the above lines and then press ENTER to continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rajarshi/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib\n",
    "import glob\n",
    "import operator\n",
    "import psutil\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, AveragePooling2D, MaxPooling2D, BatchNormalization, Conv2D, MaxPooling2D, Flatten\n",
    "from shutil import copyfile\n",
    "import cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking User Inputs for File Handling:\n",
    "\n",
    "* All the images of different classes should be stored in different folders and should rename the folders using the name of the classes for better understanding.​\n",
    "\n",
    "* These separate folders should be kept inside a folder which is known as **Source folder**.\n",
    "\n",
    "* For Manual Validation of the generated model, the user needs to have some data stored in a separate folder which will be used for validation and the user will be able to see the corresponding labels of the validation data. This folder is known as **Test Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the address of the source folder: /home/rajarshi/Documents/OCT2017 \n",
      "Enter the name of the source folder where all the classes are stored: train\n",
      "Enter the address of the test images which will be used for testing after training:  /home/rajarshi/Documents/OCT2017 /val\n"
     ]
    }
   ],
   "source": [
    "source_address = str(input(\"Enter the address of the source folder: \"))\n",
    "source_folder = str(input(\"Enter the name of the source folder where all the classes are stored: \"))\n",
    "test_address = str(input(\"Enter the address of the test images which will be used for testing after training:  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder_dir = source_address + '/' + source_folder + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = sum([len(files) for r, d, files in os.walk(source_folder_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the files in terms of their size and fetching the smallest sized image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_no = len(os.listdir(source_folder_dir))\n",
    "\n",
    "classes = []\n",
    "source_folder_dir_class = []\n",
    "image_list = []\n",
    "resized_images = []\n",
    "\n",
    "files = {}\n",
    "for i in range(0,class_no):\n",
    "    i += 1\n",
    "    classes = os.listdir(source_folder_dir)\n",
    "\n",
    "    source_folder_dir_class.append(source_folder_dir + classes[i-1] + '/')\n",
    "    \n",
    "    for filename in os.listdir(source_folder_dir_class[i-1]):\n",
    "        file = source_folder_dir_class[i-1] + filename\n",
    "        extension = \".\" + str(filename.split(\".\")[-1])\n",
    "        #print(\"The format of the images: \" + str(extension))\n",
    "        x = os.path.getsize(file) \n",
    "        files.update({file:x})\n",
    "    \n",
    "    sorted_files = sorted(files.items(), key=operator.itemgetter(1))\n",
    "    img_src = sorted_files[0][0]\n",
    "\n",
    "    image = cv2.imread(img_src)\n",
    "    #print(image.shape)\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size(height*width) of the smallest image in this dataset is 496*512\n"
     ]
    }
   ],
   "source": [
    "print(\"The size(height*width) of the smallest image in this dataset is {}*{}\".format(height,width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom vs Automated Learning:\n",
    "* **If Custom learning is selected:-**\n",
    "    1. The user will be able to **split the data** as per his/her choice to create training and validation directory.\n",
    "    2. The user can **resize all the images**. The dimension of the resized images will be feeded to CNN.\n",
    "    3. Incase of a small dataset the user can also apply **Data Augmentation** to increase the robustness of the model.\n",
    "    4. The user can **customize all hyperparameters** including - \n",
    "        * Initial Learning Rate\n",
    "        * Maximum Number of Epochs\n",
    "        * Batch Size\n",
    "        * Dropout Percentage\n",
    "    5. The user can **early stop and monitor** the training using-\n",
    "        * Validation Accuracy\n",
    "        * Validation Loss\n",
    "        * Training Accuracy\n",
    "        * Training Loss\n",
    "    6. If the user goes with **Transfer Learning**, he/she will be able to **train his/her desired number of layers**. The pretrained models are -\n",
    "        * VGG16\n",
    "        * VGG19\n",
    "        * MobileNet\n",
    "        * InceptionV3\n",
    "        * ResNet50\n",
    "    7. If the user goes with **custom model creation**, he/she will have to choose-\n",
    "        * Number of Convolution Layer\n",
    "        * Number of Filters/Kernals in 1st convolution layer\n",
    "        * Size of Filters/Kernals\n",
    "        * Number of Neuron in the dense layer\n",
    "        * Activation Function for hidden layers\n",
    "    8. The user can select his/her desired **optimizer** to train the model. The availabe optimizers are-\n",
    "        * SGD\n",
    "        * RMSprop\n",
    "        * Adagrad\n",
    "        * Adadelta\n",
    "        * Adam\n",
    "        * Adamax\n",
    "        * Nadam\n",
    "* **If Automated learning is selected:**\n",
    "     All these above parameters will be selected according to the hardware configuration and dataset. The user will only be able to select between Transfer learning and Custom Pre-built Model for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Validation Directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = source_address + '/'  + 'classify ' + str(source_folder) + '/'\n",
    "train_directory = directory + 'training' + '/'\n",
    "validation_directory = directory + 'validation' + '/'\n",
    "test_directory = directory + 'testing' + '/'\n",
    "\n",
    "# If the generated target folder is not created yet\n",
    "if not os.path.exists(directory):\n",
    "    # Create the target folder\n",
    "    os.mkdir(directory)\n",
    "# If the generated target folder is already created\n",
    "else:\n",
    "    # Overwrite the target folder\n",
    "    shutil.rmtree(directory)\n",
    "    os.mkdir(directory)\n",
    "\n",
    "os.mkdir(train_directory)\n",
    "os.mkdir(validation_directory)\n",
    "os.mkdir(test_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data:\n",
    "* The program will ask for the percentage of the data using which the user want to train his/her model.​\n",
    "\n",
    "* Depending upon the user I/p, the program creates a new directory where all the data is stored inside a folder named `classify <source_folder name>`.​\n",
    "\n",
    "* Inside this folder 2 folders are created named `training` & `validation`.\n",
    "\n",
    "* Inside these 2 folders all the classes will be stored for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, VALIDATION, TESTING, VALIDATION_SPLIT_SIZE, TEST_SPLIT_SIZE):\n",
    "    files = []\n",
    "    for filename in os.listdir(SOURCE):\n",
    "        file = SOURCE + filename\n",
    "        if os.path.getsize(file) > 0:\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            print(filename + \" is zero length, so ignoring!\")\n",
    "\n",
    "    # Splitting the Data    \n",
    "    validation_length = int(len(files) * VALIDATION_SPLIT_SIZE)\n",
    "    testing_length = int(len(files) * TEST_SPLIT_SIZE)\n",
    "    training_length = len(files) - (validation_length + testing_length)\n",
    "    \n",
    "    shuffled_set = random.sample(files, len(files))\n",
    "    \n",
    "    training_set = shuffled_set[0:training_length]\n",
    "    validation_set = shuffled_set[(training_length):(training_length+validation_length)]\n",
    "    testing_set = shuffled_set[(training_length+validation_length):(training_length+validation_length+testing_length)]\n",
    "\n",
    "    # Copying the Data\n",
    "    for filename in training_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TRAINING + filename\n",
    "        copyfile(this_file, destination)\n",
    "        \n",
    "    for filename in validation_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = VALIDATION + filename\n",
    "        copyfile(this_file, destination)\n",
    "\n",
    "    for filename in testing_set:\n",
    "        this_file = SOURCE + filename\n",
    "        destination = TESTING + filename\n",
    "        copyfile(this_file, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the splitted data to the Validation Directory & Resizing & Renaming it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using how much data(%) you want to validate? 20\n",
      "Using how much data(%) you want to test? 10\n",
      "\n",
      "Splitted the RAW data from /home/rajarshi/Documents/OCT2017 /train/DME/ & storing it into 3 folders at:\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/training/DME/\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/validation/DME/\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/testing/DME/\n",
      "\n",
      "Splitted the RAW data from /home/rajarshi/Documents/OCT2017 /train/DRUSEN/ & storing it into 3 folders at:\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/training/DRUSEN/\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/validation/DRUSEN/\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/testing/DRUSEN/\n",
      "\n",
      "Splitted the RAW data from /home/rajarshi/Documents/OCT2017 /train/NORMAL/ & storing it into 3 folders at:\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/training/NORMAL/\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/validation/NORMAL/\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/testing/NORMAL/\n",
      "\n",
      "Splitted the RAW data from /home/rajarshi/Documents/OCT2017 /train/CNV/ & storing it into 3 folders at:\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/training/CNV/\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/validation/CNV/\n",
      "/home/rajarshi/Documents/OCT2017 /classify train/testing/CNV/\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT_SIZE = int(input(\"Using how much data(%) you want to validate? \"))\n",
    "VALIDATION_SPLIT_SIZE = VALIDATION_SPLIT_SIZE/100\n",
    "\n",
    "TEST_SPLIT_SIZE = int(input(\"Using how much data(%) you want to test? \"))\n",
    "TEST_SPLIT_SIZE = TEST_SPLIT_SIZE/100\n",
    "\n",
    "train_class = []\n",
    "validation_class = []\n",
    "test_class = []\n",
    "\n",
    "for i in range(0,class_no):\n",
    "    i += 1\n",
    "\n",
    "    train_class.append(train_directory + classes[i-1] + '/')\n",
    "    validation_class.append(validation_directory + classes[i-1] + '/')\n",
    "    test_class.append(test_directory + classes[i-1] + '/')\n",
    "\n",
    "    os.mkdir(train_class[i-1])\n",
    "    os.mkdir(validation_class[i-1])\n",
    "    os.mkdir(test_class[i-1])\n",
    "\n",
    "    split_data(source_folder_dir_class[i-1], train_class[i-1], validation_class[i-1], test_class[i-1], VALIDATION_SPLIT_SIZE, TEST_SPLIT_SIZE)\n",
    "\n",
    "    print(\"\\nSplitted the RAW data from \" + str(source_folder_dir_class[i-1]) + \" & storing it into 3 folders at:\\n\" + str(train_class[i-1]) + '\\n' + str(validation_class[i-1]) + '\\n' + str(test_class[i-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You can resize the images to 496(h)*512(w) or lower than that.\n",
      "Enter the height to which you want to resize all the images: 128\n",
      "Enter the width to which you want to resize all the images: 128\n"
     ]
    }
   ],
   "source": [
    "cnn_size = 224\n",
    "\n",
    "dim = []\n",
    "resized_images = []\n",
    "\n",
    "print(\"\\n\\nYou can resize the images to {}(h)*{}(w) or lower than that.\".format(height,width))\n",
    "height = int(input(\"Enter the height to which you want to resize all the images: \"))\n",
    "width = int(input(\"Enter the width to which you want to resize all the images: \"))\n",
    "    \n",
    "for i in range(0,class_no):    \n",
    "    # Renaming the images\n",
    "    a = 0\n",
    "    for filename in os.listdir(train_class[i-1]):\n",
    "        os.rename(train_class[i-1] + filename, train_class[i-1] + \"img \" + str(a) + extension)\n",
    "        a += 1\n",
    "\n",
    "    a = 0\n",
    "    for filename in os.listdir(validation_class[i-1]):\n",
    "        os.rename(validation_class[i-1] + filename, validation_class[i-1] + \"img \" + str(a) + extension)\n",
    "        a += 1\n",
    "\n",
    "    a = 0\n",
    "    for filename in os.listdir(test_class[i-1]):\n",
    "        os.rename(test_class[i-1] + filename, test_class[i-1] + \"img \" + str(a) + extension)\n",
    "        a += 1\n",
    "        \n",
    "        \n",
    "# Data-Processing\n",
    "for i in range(0,class_no):\n",
    "    a = 0\n",
    "    for filename in os.listdir(train_class[i-1]):\n",
    "        file = os.path.join(train_class[i-1], filename)\n",
    "        \n",
    "        img = cv2.imread(file)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        ret, thresh_gray = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        gray[thresh_gray == 255] = 0\n",
    "\n",
    "        bbox = cv2.boundingRect(gray)\n",
    "        x, y, w, h = bbox\n",
    "        foreground = img[y:y+h, x:x+w]\n",
    "\n",
    "        ret, thresh_foreground = cv2.threshold(foreground, 220, 255, cv2.THRESH_BINARY)\n",
    "        foreground[thresh_foreground == 255] = 0\n",
    "        \n",
    "        res = cv2.resize(foreground, (width, height))\n",
    "        resized_images.append(res)\n",
    "        cv2.imwrite(train_class[i-1] + \"img \" + str(a) + extension, res)\n",
    "        a+= 1\n",
    "        dim = resized_images[0].shape\n",
    "\n",
    "    a = 0\n",
    "    for filename in os.listdir(validation_class[i-1]):\n",
    "        file = os.path.join(validation_class[i-1], filename)\n",
    "        \n",
    "        img = cv2.imread(file)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        ret, thresh_gray = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        gray[thresh_gray == 255] = 0\n",
    "\n",
    "        bbox = cv2.boundingRect(gray)\n",
    "        x, y, w, h = bbox\n",
    "        foreground = img[y:y+h, x:x+w]\n",
    "\n",
    "        ret, thresh_foreground = cv2.threshold(foreground, 220, 255, cv2.THRESH_BINARY)\n",
    "        foreground[thresh_foreground == 255] = 0\n",
    "        \n",
    "        res = cv2.resize(foreground, (width, height))\n",
    "        resized_images.append(res)\n",
    "        cv2.imwrite(validation_class[i-1] + \"img \" + str(a) + extension, res)\n",
    "        a+= 1\n",
    "        dim = resized_images[0].shape\n",
    "        \n",
    "    a = 0\n",
    "    for filename in os.listdir(test_class[i-1]):\n",
    "        file = os.path.join(test_class[i-1], filename)\n",
    "        \n",
    "        img = cv2.imread(file)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        ret, thresh_gray = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        gray[thresh_gray == 255] = 0\n",
    "\n",
    "        bbox = cv2.boundingRect(gray)\n",
    "        x, y, w, h = bbox\n",
    "        foreground = img[y:y+h, x:x+w]\n",
    "\n",
    "        ret, thresh_foreground = cv2.threshold(foreground, 220, 255, cv2.THRESH_BINARY)\n",
    "        foreground[thresh_foreground == 255] = 0\n",
    "        \n",
    "        res = cv2.resize(foreground, (width, height))\n",
    "        resized_images.append(res)\n",
    "        cv2.imwrite(test_class[i-1] + \"img \" + str(a) + extension, res)\n",
    "        a+= 1\n",
    "        dim = resized_images[0].shape\n",
    "        \n",
    "dim = (dim[1], dim[0], dim[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The images have been reshaped to(width * height * channel) (128, 128, 3) for the sake of normalization\n",
      "\n",
      "The  4 classes of the data are =  ['DME', 'DRUSEN', 'NORMAL', 'CNV']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThe images have been reshaped to(width * height * channel) \" + str(dim) + \" for the sake of normalization\")\n",
    "print(\"\\nThe \", class_no, \"classes of the data are = \",classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset contains total 83484 images\n",
      "Augmenting data will make the model robust but it'll take more time to train. \n",
      "Do you want to augment the data?(Y/N) n\n"
     ]
    }
   ],
   "source": [
    "augmentation = []\n",
    "print(\"This dataset contains total {} images\".format(source_data)) \n",
    "\n",
    "augment = input(str(\"Augmenting data will make the model robust but it'll take more time to train. \\nDo you want to augment the data?(Y/N) \"))\n",
    "\n",
    "if augment.upper() == 'Y':\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1/ 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "    augmentation.append('Data is Augmented')\n",
    "\n",
    "elif augment.upper() == 'N':\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1/ 255)\n",
    "    augmentation.append('Data is not Augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyparparameter Tunining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the initial learning rate: 0.0001\n",
      "Enter the maximum number of epochs: 100\n",
      "Enter batch size: 10\n"
     ]
    }
   ],
   "source": [
    "learning_rate = float(input(\"Enter the initial learning rate: \"))\n",
    "epoch = int(input(\"Enter the maximum number of epochs: \"))\n",
    "batch_size = int(input(\"Enter batch size: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE HYPERPARAMETERS ARE-\n",
      "Initial Learning Rate =  0.0001\n",
      "Maximum No. of epochs =  100\n",
      "Batch Size =  10\n"
     ]
    }
   ],
   "source": [
    "print(\"THE HYPERPARAMETERS ARE-\")\n",
    "print(\"Initial Learning Rate = \", learning_rate)\n",
    "print(\"Maximum No. of epochs = \", epoch)\n",
    "print(\"Batch Size = \", batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder for Performance Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "today = today.strftime(\"%d.%m\")\n",
    "t = time.ctime()\n",
    "t = t[11:-5]\n",
    "\n",
    "char = source_address + '/' + today + '-' + t\n",
    "os.mkdir(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks:-\n",
    "### Early Stopping:\n",
    "\n",
    "* The user can early stop the training by monitoring the any one of validation accuracy and validation loss.​\n",
    "\n",
    "* This can help the model to prevent overfeeding.​\n",
    "\n",
    "* If the monitored quality does not show any improvement for certain epochs(depends on RAM but customizable), the training will stop.\n",
    "\n",
    "### Model Checkpoint:\n",
    "\n",
    "* Saves the model after every epoch where the monitored quantity shows the best improvement.\n",
    "\n",
    "### Learning Rate Scheduler: \n",
    "\n",
    "* $decay = \\alpha * (0.1)^\\frac{epoch}{10}$, i.e. the learning rate will show a decay by a factor of 0.1 after 10 epochs, if the maximum epoch is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return learning_rate * (0.1 ** int(epoch / 10))\n",
    "    \n",
    "best_model_address = char + '/' + 'best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop prevents overfitting and sometimes it may result better output response. Do you want to early stop the training?(Y/N) y\n",
      "\n",
      "Montoring Losses will consume more time to train the model but will return a perfect model with minimum losses.\n",
      "Press 1 to monitor Validation Accuracy\n",
      "Press 2 to monitor Validation Loss\n",
      "Press 3 to monitor Training Accuracy\n",
      "Press 4 to monitor Training Loss\n",
      "4\n",
      "Enter number of epochs that will produce monitored quantity with no improvement after which training will be stopped: 5\n",
      "\n",
      "MONITORING TRAINING LOSS..........\n",
      "\n",
      "\n",
      "Training will stop if Validation Accuracy doesn't show any improvements for 5 epcohs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stop = str(input(\"Early stop prevents overfitting and sometimes it may result better output response. Do you want to early stop the training?(Y/N) \"))\n",
    "\n",
    "if early_stop.upper() == 'Y':\n",
    "    print(\"\\nMontoring Losses will consume more time to train the model but will return a perfect model with minimum losses.\")\n",
    "    monitor = int(input(\"Press 1 to monitor Validation Accuracy\\nPress 2 to monitor Validation Loss\\nPress 3 to monitor Training Accuracy\\nPress 4 to monitor Training Loss\\n\"))\n",
    "    patience = int(input('Enter number of epochs that will produce monitored quantity with no improvement after which training will be stopped: '))\n",
    "\n",
    "\n",
    "    if monitor == 1:\n",
    "        metric = 'val_accuracy'\n",
    "        mode = 'max'\n",
    "        print(\"\\nMONITORING VALIDATION ACCURACY..........\\n\")\n",
    "\n",
    "    elif monitor == 2:\n",
    "        metric = 'val_loss'\n",
    "        mode = 'min'\n",
    "        print(\"\\nMONITORING VALIDATION LOSS..........\\n\")\n",
    "\n",
    "    elif monitor == 3:\n",
    "        metric = 'accuracy'\n",
    "        mode = 'max'\n",
    "        print(\"\\nMONITORING TRAINING ACCURACY..........\\n\")\n",
    "\n",
    "    elif monitor == 4:\n",
    "        metric = 'loss'\n",
    "        mode = 'min'\n",
    "        print(\"\\nMONITORING TRAINING LOSS..........\\n\")\n",
    "\n",
    "    callback = [keras.callbacks.callbacks.LearningRateScheduler(lr_schedule, verbose = 1),\n",
    "                keras.callbacks.callbacks.EarlyStopping(monitor = metric, min_delta = 0.001, patience = patience, verbose=1, mode = mode, restore_best_weights = True),\n",
    "                keras.callbacks.callbacks.ModelCheckpoint(best_model_address, monitor = metric, verbose=1, save_best_only=True, save_weights_only=False, mode = mode , period=1)]\n",
    "\n",
    "    print(\"\\nTraining will stop if Validation Accuracy doesn't show any improvements for \" + str(patience) + \" epcohs\\n\")\n",
    "\n",
    "elif early_stop.upper() == 'N':\n",
    "    monitor = int(input(\"Press 1 to monitor Validation Accuracy\\nPress 2 to monitor Validation Loss\\n\"))\n",
    "\n",
    "    if monitor == 1:\n",
    "        metric = 'val_accuracy'\n",
    "        mode = 'max'\n",
    "        print(\"\\nMONITORING VALIDATION ACCURACY..........\\n\")\n",
    "\n",
    "    elif monitor == 2:\n",
    "        metric = 'val_loss'\n",
    "        mode = 'min'\n",
    "        print(\"\\nMONITORING VALIDATION LOSS..........\\n\")\n",
    "\n",
    "    elif monitor == 3:\n",
    "        metric = 'accuracy'\n",
    "        mode = 'max'\n",
    "        print(\"\\nMONITORING TRAINING ACCURACY..........\\n\")\n",
    "\n",
    "    elif monitor == 4:\n",
    "        metric = 'loss'\n",
    "        mode = 'min'\n",
    "        print(\"\\nMONITORING TRAINING LOSS..........\\n\")\n",
    "\n",
    "\n",
    "    callback = [keras.callbacks.callbacks.LearningRateScheduler(lr_schedule, verbose = 1),\n",
    "                keras.callbacks.callbacks.ModelCheckpoint(best_model_address, monitor = metric, verbose = 1, save_best_only = True, save_weights_only = False, mode = mode, period = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation:\n",
    "* Transfer Learning using Pre-trained models - \n",
    "    * VGG16\n",
    "    * VGG19\n",
    "    * MobileNet\n",
    "    * InceptionV3\n",
    "\n",
    "* Custom Model\n",
    "* Small Model for 64*64 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16():\n",
    "    print(\"\\nTRAINING ON VGG16 MODEL:-\")\n",
    "\n",
    "    base_model = keras.applications.vgg16.VGG16(input_shape = dim, weights = 'imagenet', include_top = False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(dense, activation=activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    predictions = Dense(output_layer, activation = output_activation)(x)\n",
    "\n",
    "    model = Model(inputs = base_model.input, outputs=predictions)\n",
    "\n",
    "    train_base_model = str(input(\"Do you want to train the base model of VGG16?(Y/N) \"))\n",
    "    if train_base_model.upper() == 'Y':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    elif train_base_model.upper() == 'N':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    print(\"\\nTRAINING ON VGG19 MODEL:-\")\n",
    "\n",
    "    base_model = keras.applications.vgg19.VGG19(input_shape = dim, weights = 'imagenet', include_top = False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(dense, activation=activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    predictions = Dense(output_layer, activation = output_activation)(x)\n",
    "\n",
    "    model = Model(inputs = base_model.input, outputs=predictions)\n",
    "\n",
    "    train_base_model = str(input(\"Do you want to train the base model of vgg19?(Y/N) \"))\n",
    "    if train_base_model.upper() == 'Y':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    elif train_base_model.upper() == 'N':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def MobileNet():\n",
    "    print(\"\\nTRAINING ON MobileNet MODEL:-\")\n",
    "\n",
    "    base_model = keras.applications.mobilenet.MobileNet(input_shape = dim, weights = 'imagenet', include_top = False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(dense, activation=activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    predictions = Dense(output_layer, activation = output_activation)(x)\n",
    "\n",
    "    model = Model(inputs = base_model.input, outputs=predictions)\n",
    "\n",
    "    train_base_model = str(input(\"Do you want to train the base model of MobileNet?(Y/N) \"))\n",
    "    if train_base_model.upper() == 'Y':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    elif train_base_model.upper() == 'N':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def InceptionV3():\n",
    "    print(\"\\nTRAINING ON InceptionV3 MODEL:-\")\n",
    "    \n",
    "    base_model = keras.applications.inception_v3.InceptionV3(input_shape = dim, weights = 'imagenet', include_top = False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(dense, activation=activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    predictions = Dense(output_layer, activation = output_activation)(x)\n",
    "\n",
    "    model = Model(inputs = base_model.input, outputs=predictions)\n",
    "\n",
    "    train_base_model = str(input(\"Do you want to train the base model of InceptionV3?(Y/N) \"))\n",
    "    if train_base_model.upper() == 'Y':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    elif train_base_model.upper() == 'N':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    print(\"\\nTRAINING ON ResNet50 MODEL:-\")\n",
    "\n",
    "    base_model = keras.applications.resnet.ResNet50(input_shape = dim, weights = 'imagenet', include_top = False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(dense, activation=activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    predictions = Dense(output_layer, activation = output_activation)(x)\n",
    "\n",
    "    model = Model(inputs = base_model.input, outputs=predictions)\n",
    "\n",
    "    train_base_model = str(input(\"Do you want to train the base model of ResNet50?(Y/N) \"))\n",
    "    if train_base_model.upper() == 'Y':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    elif train_base_model.upper() == 'N':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def Custom_Model():\n",
    "    print(\"\\nTRAINING ON A COMPLEX CUSTOM MODEL:-\")\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    for l in range(layer):\n",
    "        l += 1\n",
    "        m = (2**l)//2\n",
    "        for c in range(conv_layer):\n",
    "            model.add(Conv2D(conv*m, (conv_size, conv_size), padding = 'same', input_shape = dim, activation = activation))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense, activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(output_layer, activation=output_activation))\n",
    "    return model\n",
    "\n",
    "\n",
    "def Pretrained_Custom_Model():\n",
    "    model_address = str(input(\"Enter the address of the model: \"))\n",
    "    model_name = str(input(\"Enter the name of the model: \"))\n",
    "    model_dir = os.path.join(model_address, model_name)\n",
    "    \n",
    "    model = load_model(model_dir)\n",
    "    model.summary()\n",
    "    \n",
    "    train_layers = str(input(\"Do you want to train all layers? \"))\n",
    "    if train_layers.upper() == 'Y':\n",
    "        for layers in model.layers:\n",
    "            layers.trainable = True\n",
    "    \n",
    "    elif train_layers.upper() == 'N':\n",
    "        trainables = int(input(\"How many number of layers you want to train? \"))\n",
    "        for layers in model.layers[:-trainables]:\n",
    "            layers.trainable = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 4-Class Classification\n"
     ]
    }
   ],
   "source": [
    "if class_no > 2:\n",
    "    print(\"This is a \" + str(class_no) + \"-Class Classification\")\n",
    "    output_activation = 'softmax'\n",
    "    losses = 'categorical_crossentropy'\n",
    "    class_mode = 'categorical'\n",
    "    output_layer = class_no\n",
    "else:\n",
    "    print(\"This is a Binary Classification\")\n",
    "    output_activation = 'sigmoid'\n",
    "    losses = 'binary_crossentropy'\n",
    "    class_mode = 'binary'\n",
    "    output_layer = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Selection:\n",
    "The user can select between\n",
    "1. SGD\n",
    "2. RMSprop\n",
    "3. Adagrad\n",
    "4. Adadelta\n",
    "5. Adam\n",
    "6. Adamax\n",
    "7. Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_selection():\n",
    "    print(\"CTRL + Click here https://towardsdatascience.com/optimization-algorithms-in-deep-learning-191bfc2737a4 to know about optimizers.\")\n",
    "    print(\"\\nSelect a optimizer which will reduce the loss of the model.\\n\")\n",
    "\n",
    "    optimizer_select = int(input(\"Press 1 to select Stochastic Gradient Descent\\nPress 2 to select RMSprop\\nPress 3 to select Adagrad\\nPress 4 to select Adadelta\\nPress 5 to select Adam\\nPress 6 to select Adamax\\nPress 7 to select Nadam\\n\"))\n",
    "\n",
    "    if optimizer_select == 1:\n",
    "        optimizer = keras.optimizers.SGD(lr = learning_rate, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "    elif optimizer_select == 2:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate, rho = 0.9)\n",
    "\n",
    "    elif optimizer_select == 3:\n",
    "        optimizer = keras.optimizers.Adagrad(learning_rate)\n",
    "\n",
    "    elif optimizer_select == 4:\n",
    "        optimizer = keras.optimizers.Adadelta(learning_rate, rho = 0.95)\n",
    "\n",
    "    elif optimizer_select == 5:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "\n",
    "    elif optimizer_select == 6:\n",
    "        optimizer = keras.optimizers.Adamax(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999)\n",
    "\n",
    "    elif optimizer_select == 7:\n",
    "        optimizer = keras.optimizers.Nadam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999)\n",
    "   \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58442 images belonging to 4 classes.\n",
      "Found 16696 images belonging to 4 classes.\n",
      "Found 8346 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = train_directory\n",
    "train_datagen = datagen\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    class_mode = class_mode,\n",
    "                                                    target_size = (dim[0],dim[1]),\n",
    "                                                    shuffle=True)\n",
    "\n",
    "VALIDATION_DIR = validation_directory\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
    "                                                              batch_size = batch_size,\n",
    "                                                              class_mode = class_mode,\n",
    "                                                              target_size = (dim[0],dim[1]),\n",
    "                                                              shuffle=True)\n",
    "\n",
    "TESTING_DIR = test_directory\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "test_generator = test_datagen.flow_from_directory(TESTING_DIR,\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  class_mode = class_mode,\n",
    "                                                  target_size = (dim[0],dim[1]),\n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = validation_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Press 1 to train using pre-trained VGG16 Model\n",
      "Press 2 to train using pre-trained VGG19 Model\n",
      "Press 3 to train using pre-trained MobilNet Model\n",
      "Press 4 to train using pre-trained InceptionV3 Model\n",
      "Press 5 to train using pre-trained ResNet50 Model\n",
      "Press 6 to create your own Model\n",
      "Press 7 to train using previously trained Custom Model\n",
      "7\n",
      "CTRL + Click here https://towardsdatascience.com/optimization-algorithms-in-deep-learning-191bfc2737a4 to know about optimizers.\n",
      "\n",
      "Select a optimizer which will reduce the loss of the model.\n",
      "\n",
      "Press 1 to select Stochastic Gradient Descent\n",
      "Press 2 to select RMSprop\n",
      "Press 3 to select Adagrad\n",
      "Press 4 to select Adadelta\n",
      "Press 5 to select Adam\n",
      "Press 6 to select Adamax\n",
      "Press 7 to select Nadam\n",
      "5\n",
      "Enter the address of the model: /home/rajarshi/Documents/IEEE Conf/Final_BEST\n",
      "Enter the name of the model: best_model.h5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 2,228,628\n",
      "Trainable params: 2,228,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Do you want to train all layers? n\n",
      "How many number of layers you want to train? 3\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 2,228,628\n",
      "Trainable params: 1,049,860\n",
      "Non-trainable params: 1,178,768\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 547s 94ms/step - loss: 0.1192 - accuracy: 0.9690 - val_loss: 0.0064 - val_accuracy: 0.9635\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.11921, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 499s 85ms/step - loss: 0.0873 - accuracy: 0.9729 - val_loss: 0.0046 - val_accuracy: 0.9625\n",
      "\n",
      "Epoch 00002: loss improved from 0.11921 to 0.08730, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 475s 81ms/step - loss: 0.0724 - accuracy: 0.9772 - val_loss: 0.0109 - val_accuracy: 0.9619\n",
      "\n",
      "Epoch 00003: loss improved from 0.08730 to 0.07241, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 475s 81ms/step - loss: 0.0596 - accuracy: 0.9806 - val_loss: 0.0076 - val_accuracy: 0.9625\n",
      "\n",
      "Epoch 00004: loss improved from 0.07241 to 0.05959, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5845/5845 [==============================] - 462s 79ms/step - loss: 0.0489 - accuracy: 0.9845 - val_loss: 1.8669e-04 - val_accuracy: 0.9618\n",
      "\n",
      "Epoch 00005: loss improved from 0.05959 to 0.04893, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 461s 79ms/step - loss: 0.0412 - accuracy: 0.9874 - val_loss: 0.1388 - val_accuracy: 0.9601\n",
      "\n",
      "Epoch 00006: loss improved from 0.04893 to 0.04124, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 463s 79ms/step - loss: 0.0341 - accuracy: 0.9896 - val_loss: 0.0032 - val_accuracy: 0.9590\n",
      "\n",
      "Epoch 00007: loss improved from 0.04124 to 0.03411, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 463s 79ms/step - loss: 0.0282 - accuracy: 0.9918 - val_loss: 0.0303 - val_accuracy: 0.9605\n",
      "\n",
      "Epoch 00008: loss improved from 0.03411 to 0.02823, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 461s 79ms/step - loss: 0.0238 - accuracy: 0.9934 - val_loss: 1.3299e-04 - val_accuracy: 0.9607\n",
      "\n",
      "Epoch 00009: loss improved from 0.02823 to 0.02379, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.0001.\n",
      "5845/5845 [==============================] - 464s 79ms/step - loss: 0.0198 - accuracy: 0.9946 - val_loss: 0.0025 - val_accuracy: 0.9587\n",
      "\n",
      "Epoch 00010: loss improved from 0.02379 to 0.01979, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 464s 79ms/step - loss: 0.0152 - accuracy: 0.9962 - val_loss: 0.0208 - val_accuracy: 0.9598\n",
      "\n",
      "Epoch 00011: loss improved from 0.01979 to 0.01517, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 462s 79ms/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 0.3806 - val_accuracy: 0.9603\n",
      "\n",
      "Epoch 00012: loss improved from 0.01517 to 0.01361, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 462s 79ms/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 0.0018 - val_accuracy: 0.9608\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.01361\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 466s 80ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 7.1339e-05 - val_accuracy: 0.9603\n",
      "\n",
      "Epoch 00014: loss improved from 0.01361 to 0.01286, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 468s 80ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 0.0127 - val_accuracy: 0.9607\n",
      "\n",
      "Epoch 00015: loss improved from 0.01286 to 0.01244, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 473s 81ms/step - loss: 0.0119 - accuracy: 0.9970 - val_loss: 9.2771e-04 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00016: loss improved from 0.01244 to 0.01194, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 464s 79ms/step - loss: 0.0120 - accuracy: 0.9970 - val_loss: 3.3695e-04 - val_accuracy: 0.9599\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.01194\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 472s 81ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 8.6601e-04 - val_accuracy: 0.9602\n",
      "\n",
      "Epoch 00018: loss improved from 0.01194 to 0.01125, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 470s 80ms/step - loss: 0.0109 - accuracy: 0.9974 - val_loss: 1.0056 - val_accuracy: 0.9602\n",
      "\n",
      "Epoch 00019: loss improved from 0.01125 to 0.01091, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 1e-05.\n",
      "5845/5845 [==============================] - 462s 79ms/step - loss: 0.0109 - accuracy: 0.9973 - val_loss: 9.5911e-05 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00020: loss improved from 0.01091 to 0.01089, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "5845/5845 [==============================] - 462s 79ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 2.9601e-05 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00021: loss improved from 0.01089 to 0.01050, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "5845/5845 [==============================] - 466s 80ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.0853 - val_accuracy: 0.9601\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.01050\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "5845/5845 [==============================] - 465s 80ms/step - loss: 0.0101 - accuracy: 0.9977 - val_loss: 4.5721 - val_accuracy: 0.9601\n",
      "\n",
      "Epoch 00023: loss improved from 0.01050 to 0.01009, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "5845/5845 [==============================] - 466s 80ms/step - loss: 0.0099 - accuracy: 0.9975 - val_loss: 5.8809e-06 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00024: loss improved from 0.01009 to 0.00989, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "5845/5845 [==============================] - 466s 80ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 7.0665e-04 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.00989\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "5845/5845 [==============================] - 465s 80ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.8089 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.00989\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "5845/5845 [==============================] - 464s 79ms/step - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.0517 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00027: loss improved from 0.00989 to 0.00988, saving model to /home/rajarshi/Documents/OCT2017 /28.03-22:46:30/best_model.h5\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 1.0000000000000002e-06.\n",
      "5845/5845 [==============================] - 462s 79ms/step - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.0015 - val_accuracy: 0.9601\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.00988\n",
      "Epoch 00028: early stopping\n",
      "5845/5845 [==============================] - 335s 57ms/step\n",
      "1670/1670 [==============================] - 96s 57ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835/835 [==============================] - 46s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "tl_models = int(input(\"\\nPress 1 to train using pre-trained VGG16 Model\\nPress 2 to train using pre-trained VGG19 Model\\nPress 3 to train using pre-trained MobilNet Model\\nPress 4 to train using pre-trained InceptionV3 Model\\nPress 5 to train using pre-trained ResNet50 Model\\nPress 6 to create your own Model\\nPress 7 to train using previously trained Custom Model\\n\"))\n",
    "    \n",
    "optimizer = optimizer_selection()\n",
    "    \n",
    "if tl_models == 7:\n",
    "    model = Pretrained_Custom_Model() \n",
    "    \n",
    "else:\n",
    "    dense = int(input(\"Enter the no. of neurons in dense layer: \"))\n",
    "    activation = str(input(\"Enter the activation function: \"))\n",
    "    dropout = float(input(\"Enter the dropout percentage: \"))\n",
    "    dropout = dropout/100\n",
    "\n",
    "    if tl_models == 1: \n",
    "        model= vgg16()\n",
    "\n",
    "    elif tl_models == 2:\n",
    "        model = vgg19()\n",
    "\n",
    "    elif tl_models == 3:\n",
    "        model = MobileNet()\n",
    "\n",
    "    elif tl_models == 4:\n",
    "        model = InceptionV3() \n",
    "\n",
    "    elif tl_models == 5:\n",
    "        model = ResNet50()\n",
    "\n",
    "    elif tl_models == 6:\n",
    "        print(\"\\nThe images have been resized to \" + str(dim))\n",
    "        layer = int(input(\"Enter number of layers you want to apply: \"))\n",
    "        conv_layer = int(input(\"Enter number of convolution layers you want to apply: \"))\n",
    "        conv = int(input(\"Enter the no. of filters in the 1st convolution layer: \"))\n",
    "        conv_size = int(input(\"Enter the size of filters: \"))\n",
    "\n",
    "        model = Custom_Model()\n",
    "    \n",
    "    \n",
    "model.compile(optimizer = optimizer, loss = losses, metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator,\n",
    "                                    epochs = epoch,\n",
    "                                    verbose = 1,\n",
    "                                    callbacks = callback,\n",
    "                                    validation_data = validation_generator,\n",
    "                                    shuffle=True)\n",
    "\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "\n",
    "train_score = model.evaluate(train_generator)\n",
    "val_score = model.evaluate(validation_generator)\n",
    "test_score = model.evaluate(test_generator)\n",
    "\n",
    "model_weight = \"weight.h5\"\n",
    "model_weight_address = char + '/' + model_weight\n",
    "model.save_weights(model_weight_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 3.653678761455748 hours\n",
      "Model created at /home/rajarshi/Documents/OCT2017 \n"
     ]
    }
   ],
   "source": [
    "if duration < 60:\n",
    "    print(\"Execution Time:\",duration,\"seconds\")\n",
    "elif duration > 60 and duration < 3600:\n",
    "    duration=duration/60\n",
    "    print(\"Execution Time:\",duration,\"minutes\")\n",
    "else:\n",
    "    duration=duration/(60*60)\n",
    "    print(\"Execution Time:\",duration,\"hours\")\n",
    "\n",
    "print('Model created at ' + source_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing of the Model:\n",
    "The program will label the images from the test folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = source_address + '/outputs' + '/'\n",
    "\n",
    "# If the generated target folder is not created yet\n",
    "if not os.path.exists(output_dir):\n",
    "    # Create the target folder\n",
    "    shutil.copytree(test_address, output_dir)\n",
    "# If the generated target folder is already created\n",
    "else:\n",
    "    # Overwrite the target folder\n",
    "    shutil.rmtree(output_dir)\n",
    "    shutil.copytree(test_address, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(output_dir):\n",
    "    file = os.path.join(output_dir, filename)\n",
    "    img = cv2.imread(file)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    ret, thresh_gray = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    gray[thresh_gray == 255] = 0\n",
    "\n",
    "    bbox = cv2.boundingRect(gray)\n",
    "    x, y, w, h = bbox\n",
    "    foreground = img[y:y+h, x:x+w]\n",
    "\n",
    "    ret, thresh_foreground = cv2.threshold(foreground, 220, 255, cv2.THRESH_BINARY)\n",
    "    foreground[thresh_foreground == 255] = 0\n",
    "\n",
    "    res = cv2.resize(foreground, (width, height))\n",
    "    resized_images.append(res)\n",
    "    cv2.imwrite(output_dir + \"img \" + str(a) + extension, res)\n",
    "    a+= 1\n",
    "    dim = resized_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pred(pred):\n",
    "    pred_categorical = keras.utils.to_categorical(pred)\n",
    "    if class_no > 2:\n",
    "        pred_max = np.argmax(pred)\n",
    "    else:\n",
    "        pred_max = np.argmax(pred_categorical)\n",
    "    return pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL-5193994-1.jpeg belongs to class - NORMAL\n",
      "CNV-6851127-1.jpeg belongs to class - DME\n",
      "NORMAL-4872585-1.jpeg belongs to class - NORMAL\n",
      "CNV-8598714-1.jpeg belongs to class - CNV\n",
      "NORMAL-5324912-1.jpeg belongs to class - NORMAL\n",
      "DRUSEN-9800172-2.jpeg belongs to class - DRUSEN\n",
      "CNV-6294785-1.jpeg belongs to class - CNV\n",
      "CNV-6294785-2.jpeg belongs to class - DME\n",
      "DRUSEN-9928043-1.jpeg belongs to class - DRUSEN\n",
      "DME-9655949-1.jpeg belongs to class - DME\n",
      "NORMAL-5246808-2.jpeg belongs to class - NORMAL\n",
      "CNV-8184974-1.jpeg belongs to class - CNV\n",
      "DME-9583225-2.jpeg belongs to class - DME\n",
      "CNV-6875371-1.jpeg belongs to class - DME\n",
      "DRUSEN-9861332-1.jpeg belongs to class - DRUSEN\n",
      "CNV-6668596-1.jpeg belongs to class - CNV\n",
      "DME-9925591-2.jpeg belongs to class - CNV\n",
      "DRUSEN-9894035-1.jpeg belongs to class - NORMAL\n",
      "DME-9603124-1.jpeg belongs to class - DME\n",
      "DME-9721607-2.jpeg belongs to class - DME\n",
      "NORMAL-5246808-1.jpeg belongs to class - NORMAL\n",
      "NORMAL-5156112-1.jpeg belongs to class - NORMAL\n",
      "DRUSEN-9884539-1.jpeg belongs to class - DRUSEN\n",
      "DRUSEN-9894035-2.jpeg belongs to class - DME\n",
      "DME-9721607-1.jpeg belongs to class - DME\n",
      "NORMAL-5171640-1.jpeg belongs to class - NORMAL\n",
      "DRUSEN-9884539-2.jpeg belongs to class - DRUSEN\n",
      "CNV-6652117-1.jpeg belongs to class - CNV\n",
      "DME-9583225-1.jpeg belongs to class - DME\n",
      "DRUSEN-9837663-1.jpeg belongs to class - DRUSEN\n",
      "DME-9925591-1.jpeg belongs to class - DME\n",
      "NORMAL-9053621-1.jpeg belongs to class - NORMAL\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from shutil import copyfile\n",
    "\n",
    "a = 0\n",
    "r = 0\n",
    "\n",
    "for filename in os.listdir(test_address):\n",
    "    file = output_dir + filename\n",
    "    img = image.load_img(file, target_size=(dim[1], dim[0]))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    images = np.vstack([x])\n",
    "\n",
    "    pred = model.predict(images, batch_size = batch_size)\n",
    "    #pred_categorical = keras.utils.to_categorical(pred)\n",
    "    pred_max = np.argmax(pred, axis = 1)\n",
    "    #pred_max = max_pred(pred)\n",
    "\n",
    "    for r in range(class_no):\n",
    "        if pred_max == labels[classes[r]]:\n",
    "            name = classes[r]\n",
    "        elif pred_max == labels[classes[r]]:\n",
    "            name = classes[r]\n",
    "\n",
    "    os.rename(output_dir + \"/\" + filename, output_dir + \"/\" + str(name) + \"_\" + str(a) + extension)\n",
    "    a += 1\n",
    "\n",
    "    test_predictions = str(filename) + ' belongs to class - ' + str(name)\n",
    "    print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristics of the Model\n",
    "It will show the Accuracy vs Epoch and Loss vs Epoch curves and the curves will be saved in the dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Training and validation accuracy vs Epochs')\n",
    "plt.legend()\n",
    "accuracy_fig_name = \"accuracy.png\"\n",
    "plt.savefig(char + '/' + accuracy_fig_name)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Training and validation loss vs Epochs')\n",
    "plt.legend()\n",
    "loss_fig_name = \"loss.png\"\n",
    "plt.savefig(char + '/' + loss_fig_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: 99.82717633247375 %\n",
      "The validation accuracy is: 96.01101875305176 %\n",
      "The test accuracy is: 96.66906595230103 %\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = train_score[1]*100\n",
    "validation_accuracy = val_score[1]*100\n",
    "test_accuracy = test_score[1]*100\n",
    "\n",
    "print(\"The training accuracy is: \" + str(training_accuracy) + ' %')\n",
    "print(\"The validation accuracy is: \" + str(validation_accuracy) + ' %')\n",
    "print(\"The test accuracy is: \" + str(test_accuracy) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating CLASSIFICATION REPORT..........:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CNV       0.98      0.97      0.97      3720\n",
      "         DME       0.90      0.94      0.92      1134\n",
      "      DRUSEN       0.85      0.87      0.86       861\n",
      "      NORMAL       0.96      0.95      0.96      2631\n",
      "\n",
      "    accuracy                           0.95      8346\n",
      "   macro avg       0.92      0.93      0.93      8346\n",
      "weighted avg       0.95      0.95      0.95      8346\n",
      "\n",
      "\n",
      "Calculating SENSITIVITY & SPECIFICITY..........:\n",
      "sensitivity = 0.9939\n",
      "specificity = 0.9825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = []\n",
    "y_true = test_generator.classes\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for filename in os.listdir(test_class[i-1]):\n",
    "        file = os.path.join(test_class[i-1], filename)\n",
    "        img = image.load_img(file, target_size=(dim[1], dim[0]))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        images = np.vstack([x])\n",
    "\n",
    "        pred = model.predict(images, batch_size = batch_size)\n",
    "        pred_max = max_pred(pred)\n",
    "        #print(pred_max)\n",
    "        \n",
    "        y_pred.append(pred_max)\n",
    "\n",
    "        \n",
    "y_pred_rev =  [ele for ele in reversed(y_pred)] \n",
    "      \n",
    "print(\"Calculating CLASSIFICATION REPORT..........:\")\n",
    "classification_report = classification_report(y_true, y_pred, target_names=labels)\n",
    "print(classification_report)\n",
    "\n",
    "print(\"\\nCalculating SENSITIVITY & SPECIFICITY..........:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "total = sum(sum(cm))\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "print(\"sensitivity = {:.4f}\".format(sensitivity))\n",
    "print(\"specificity = {:.4f}\".format(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please read the text file named readme.txt for detailed information of the model.\n"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "readme_name_text = \"readme.txt\"\n",
    "print(\"Please read the text file named \" + readme_name_text + \" for detailed information of the model.\")\n",
    "\n",
    "completeName_txt = os.path.join(char, readme_name_text) \n",
    "\n",
    "readme = open(completeName_txt, \"w\")\n",
    "\n",
    "if class_no > 2:\n",
    "    readme.write(\"This is a MULTICLASS CLASSIFICATION\")\n",
    "else:\n",
    "    readme.write(\"This is a BINARY CLASSIFICATION\")\n",
    "readme.write(\"\\n\" + str(int(VALIDATION_SPLIT_SIZE*100)) + \"% data of the RAW Dataset is used for validation\")\n",
    "readme.write(\"\\n\" + str(int(TEST_SPLIT_SIZE*100)) + \"% data of the RAW Dataset is used for testing\")\n",
    "readme.write(\"\\nThe \" + str(source_folder) + \" dataset has \" + str(class_no) + \" classes\")\n",
    "readme.write(\"\\nThe classes are = \" + str(classes))\n",
    "readme.write(\"\\nThe labels are:-\")\n",
    "\n",
    "for z in range(class_no):\n",
    "    label = str(labels[classes[z]]) + \" is labelled for \" + str(classes[z])\n",
    "    readme.write(\"\\n\" + str(label))\n",
    "\n",
    "readme.write(\"\\n\\n\\n--HYPERPARAMETERS--\\n\")\n",
    "readme.write(str(augmentation))\n",
    "readme.write(\"\\nInitial Learning Rate = \" + str(learning_rate))\n",
    "readme.write(\"\\nMaximum No. of epochs = \" + str(epoch))\n",
    "readme.write(\"\\nBatch Size = \" + str(batch_size))\n",
    "readme.write(\"\\nA model has been created at \" + str(source_address))\n",
    "\n",
    "readme.write(\"\\n\\n\\n--MODEL-PARAMETERS--\")\n",
    "if monitor == 1:\n",
    "    readme.write(\"\\nValidation Accuracy of the model has been monitored.\")\n",
    "elif monitor == 2:\n",
    "    readme.write(\"\\nValidation Loss of the model has been monitored.\")\n",
    "elif monitor == 3:\n",
    "    readme.write(\"\\nTraining Accuracy of the model has been monitored.\")\n",
    "elif monitor == 4:\n",
    "    readme.write(\"\\nTraining Loss of the model has been monitored.\")\n",
    "\n",
    "#readme.write(\"\\nActivation Function = \" + str(activation))\n",
    "readme.write(\"\\nActivation Function = relu\")\n",
    "#readme.write(\"\\nDropout = \" + str(int(dropout*100)) + \"%\")\n",
    "readme.write(\"\\nDropout = 30%\")\n",
    "readme.write(\"\\nActivation function of the output layer = \" + str(output_activation))\n",
    "readme.write(\"\\nCost function of the model = \" + str(losses))\n",
    "readme.write(\"\\nOptimizer = \" + str(optimizer) + \"\\n\\n\")\n",
    "\n",
    "if tl_models == 1:\n",
    "    readme.write(\"Trained on VGG16\\n\")\n",
    "    with redirect_stdout(readme):\n",
    "        model.summary()\n",
    "\n",
    "if tl_models == 2:\n",
    "    readme.write(\"Trained on VGG19\\n\")\n",
    "    with redirect_stdout(readme):\n",
    "        model.summary()\n",
    "\n",
    "if tl_models == 3:\n",
    "    readme.write(\"Trained on MobileNet\\n\")\n",
    "    with redirect_stdout(readme):\n",
    "        model.summary()\n",
    "        \n",
    "if tl_models == 4:\n",
    "    readme.write(\"Trained on ImageNet\\n\")\n",
    "    with redirect_stdout(readme):\n",
    "        model.summary()\n",
    "        \n",
    "if tl_models == 5:\n",
    "    readme.write(\"Trained on a ResNet Model\\n\")\n",
    "    with redirect_stdout(readme):\n",
    "        model.summary() \n",
    "        \n",
    "if tl_models == 6:\n",
    "    readme.write(\"Trained on a Custom Model\\n\")\n",
    "    readme.write(\"Number of layer = \" + str(layer))\n",
    "    readme.write(\"Number of Conv layer = \" + str(conv_layer))\n",
    "    readme.write(\"\\nFilter size = \" + str(conv_size)+ \"*\" + str(conv_size) + \"\\n\\n\")\n",
    "    with redirect_stdout(readme):\n",
    "        model.summary()\n",
    "\n",
    "if tl_models == 8:\n",
    "    readme.write(\"Trained using Transfer Learning on a Custom Pre-Trained Model\\n\")\n",
    "    with redirect_stdout(readme):\n",
    "        model.summary()\n",
    "    \n",
    "readme.write(\"\\n--MODEL-PERFORMANCE--\")\n",
    "readme.write(\"\\nTraining Accuracy = \" + str(training_accuracy) + \" %\")\n",
    "readme.write(\"\\nValidation Accuracy = \" + str(validation_accuracy) + \" %\")\n",
    "readme.write(\"\\nTest Accuracy = \" + str(test_accuracy) + \" %\\n\\n\")\n",
    "\n",
    "readme.write(\"Classification Report:\\n\")\n",
    "readme.write(classification_report)\n",
    "\n",
    "readme.write(\"\\nSensitivity = \" + str(int(sensitivity*100)) + \" %\")\n",
    "readme.write(\"\\nSpecificity = \" + str(int(specificity*100)) + \" %\")\n",
    "\n",
    "readme.write(\"\\n\\nCreated using Self-Regulated Image Classifier using Convolution Neural Network\")\n",
    "\n",
    "readme.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test.py\n",
    "A python program named `test.py` will be created using which the user will be able to test his own images by feeding them into the test folder only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_name_py = \"test.py\"\n",
    "\n",
    "completeName_py = os.path.join(source_address, test_name_py) \n",
    "\n",
    "test = open(completeName_py, \"w\")\n",
    "\n",
    "test.write('import numpy as np')\n",
    "test.write('\\nimport os')\n",
    "test.write('\\nimport keras')\n",
    "test.write('\\nfrom keras.models import load_model')\n",
    "test.write('\\nimport shutil')\n",
    "test.write('\\nimport cv2')\n",
    "test.write('\\nimport glob')\n",
    "\n",
    "source_address_ = '\"{}\"'.format(source_address)\n",
    "test_address_ = '\"{}\"'.format(test_address)\n",
    "extension_ = '\"{}\"'.format(extension)\n",
    "best_model_address = '\"{}\"'.format(best_model_address)\n",
    "\n",
    "test.write('\\n\\nsource_address = ' + str(source_address_))\n",
    "test.write('\\ntest_address = ' + str(test_address_))\n",
    "test.write('\\n\\noutput_dir = source_address + \"/outputs\" + \"/\"')\n",
    "\n",
    "test.write('\\n\\nif not os.path.exists(output_dir):')\n",
    "test.write('\\n    shutil.copytree(test_address, output_dir)')\n",
    "test.write('\\nelse:')\n",
    "test.write('\\n    shutil.rmtree(output_dir)')\n",
    "test.write('\\n    shutil.copytree(test_address, output_dir)')\n",
    "\n",
    "test.write('\\n\\nwidth = ' + str(dim[0]))\n",
    "test.write('\\nheight = ' + str(dim[1]))\n",
    "test.write('\\nresized_images = []')\n",
    "\n",
    "test.write('\\n\\nfor img in glob.glob(output_dir + \"/*\" + ' + extension_ + '):')\n",
    "test.write('\\n    image = cv2.imread(img)')\n",
    "test.write('\\n    res = cv2.resize(image, (width, height))')\n",
    "test.write('\\n    resized_images.append(res)')\n",
    "test.write('\\n    dim = resized_images[0].shape')\n",
    "\n",
    "test.write('\\n\\na = 0')\n",
    "test.write('\\nr = 0')\n",
    "test.write('\\nclass_no = ' + str(class_no))\n",
    "test.write('\\nclasses = ' + str(classes))\n",
    "test.write('\\nlabels = ' + str(labels))\n",
    "test.write('\\nbatch_size = ' + str(batch_size))\n",
    "test.write('\\n\\nmodel = load_model(best_model_address)')\n",
    "test.write('\\n\\nfrom keras.preprocessing import image')\n",
    "\n",
    "test.write('\\n\\nfor filename in os.listdir(test_address):')\n",
    "test.write('\\n    file = output_dir + filename')\n",
    "test.write('\\n    img = image.load_img(file, target_size = (width, height))')\n",
    "test.write('\\n    x = image.img_to_array(img)')\n",
    "test.write('\\n    x = np.expand_dims(x, axis=0)')\n",
    "\n",
    "test.write('\\n    images = np.vstack([x])')\n",
    "test.write('\\n    pred = model.predict(images, batch_size = batch_size)')\n",
    "test.write('\\n    pred_categorical = keras.utils.to_categorical(pred)')\n",
    "\n",
    "if class_no > 2:\n",
    "    test.write('\\n    pred_max = np.argmax(pred)')\n",
    "else:\n",
    "    test.write('\\n    pred_max = np.argmax(pred_categorical)')\n",
    "    \n",
    "\n",
    "test.write('\\n\\n    for r in range(class_no):')\n",
    "test.write('\\n        if pred_max == labels[classes[r]]:')\n",
    "test.write('\\n            name = classes[r]')\n",
    "test.write('\\n        elif pred_max == labels[classes[r]]:')\n",
    "test.write('\\n            name = classes[r]')\n",
    "\n",
    "test.write('\\n\\n    os.rename(output_dir + \"/\" + filename, output_dir + \"/\" + str(name) + \"_\" + str(a) + ' + extension_ + ')')\n",
    "test.write('\\n    a += 1')\n",
    "\n",
    "test.write('\\n\\n    test_predictions = str(filename) + \" belongs to class - \" + str(name)')\n",
    "test.write('\\n    print(test_predictions)')\n",
    "\n",
    "test.write('\\n\\nprint(\"Checkout to output folder to check the outputs\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
